---
title: "Data Tidying and Cleaning"
author: "Aaron Till and Andrew Bray"
output: github_document
--- 

This document reads in several files, cleans them, and merges them into two primary
data files: a historical data set (thermal metrics and MME info pre-2014) and a future
data set (thermal metrics forcast over two periods in the 21st century). The raw files
include:

- `fill_kill_data_10_24_2018.csv`: data on MME events in Wisconsin lakes between 2003 and 2013.
- `thermal_metrics.csv`: historical thermal metric data on upper midwest lakes between 1980 and 2013 from Winslow et al.
- `ACCESS_thermal_metrics.tsv`: forcasted thermal metric data for upper midwest lakes for two periods in the 21st century.
- `NHD_WBIC.csv`: reference table to match lake ids (wbic) with site ids (some lakes have multiple sites).
- `lake_shapes`: shapefiles of wisconsin lakes (provide lat/lon).
- `census_data`: census data files to match population estimates to each lake via lat/lon.
- `PRISM_precip`: snow and precipitation data.

Some of the files are several GB in size, so it recommended that this be run on
a machine with > 20GB RAM.

# Part I: Historical Data

## Data import

```{r installing packages}
library(rgdal)
library(stringr)
library(lubridate)
library(raster)
library(tidyverse)
library(naniar)
```

```{r load data}
setwd('data/raw')
MME <-read_csv("fish_kill_data_10_24_2018.csv",
               col_types = list(WBIC = col_character(),
                                Year = col_character()))
thermal <-read_csv("thermal_metrics.csv")
NHD_WBIC <-read_csv("NHD_WBIC.csv",
                    col_types = list(WBIC = col_character())) %>%
  rename_all(tolower)
thermal <- inner_join(thermal, NHD_WBIC, by = "site_id")
```


## Cleaning the raw data

### MME raw data

First we strip out unneeded columns and tidy. There are a few instances of multiple
MMEs being recorded in the same lake in the same month, we treat as just a single 
MME event.

```{r}
MME <- MME %>%
  rename_all(tolower) %>%
  mutate_all(tolower) %>%
  dplyr::filter(min.kill.size!="excludable") %>%
  dplyr::select(wbic,
                year,
                investigation.start.month,
                fishkill.inv.seq.no,
                cause.category.4) %>%
  rename(month = investigation.start.month) %>%
  mutate(dummy = 1,
         cause.categories = cause.category.4) %>% 
  spread(cause.categories, dummy, fill = 0) %>%
  rename(anthropogenic = `anthropogenic condition`,
         infectious    = `infectious agent`) %>%
 dplyr::select(-fishkill.inv.seq.no) %>%
  distinct(wbic, year, month, .keep_all = TRUE)
```

For the thermal data, we filter out the data that predates the coverage of the 
MME data and convert to character to aid in later merges.

### Thermal raw data

```{r}
thermal <- thermal %>%
  filter(Year >= 2003) %>%
  rename_all(tolower) %>%
  mutate(year = as.character(year)) %>%
 dplyr::select(-contains("strat"),
         -sthermo_depth_mean)
```

For now, we process the annual and monthly thermal data separately. This is for
the sake of computational efficiency; they are merged later. For both the monthly
and the annual data, we average the data over site_ids (for those lakes with
multiple sites).

```{r}
thermal_annual <- thermal %>%
  dplyr::select(-contains('jas'),
                -starts_with('mean_surf_'),
                -starts_with('mean_bot_'), 
                -starts_with('max_surf_'), 
                -starts_with('max_bot_')) %>%
  group_by(wbic, year) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup() %>%
  rename(ice_duration = ice_duration_days,
         schmidt = schmidt_daily_annual_sum,
         variance_after_ice_30 = coef_var_0.30, 
         variance_after_ice_60 = coef_var_30.60, 
         cumulative_above_0 = gdd_wtr_0c,
         cumulative_above_5 = gdd_wtr_5c,
         cumulative_above_10 = gdd_wtr_10c) %>%
  mutate(log_schmidt = log(schmidt + .00001))
```

For the monthly data, the month is extracted from the variable name and turned 
into a datetime. Two additional types of features are made:

- z-scores for each of the temperature variables, subtracting off the lake x month
average and dividing by the sd.
- the temperature data is synthesized into a single PC called `temp`.

```{r}
thermal_monthly <- thermal %>%
  dplyr::select(starts_with('mean_surf_'),            # tidy the temp data
                starts_with('mean_bot_'), 
                starts_with('max_surf_'), 
                starts_with('max_bot_'),
                -contains('jas'),
                year, wbic) %>%
  mutate(uniqueid = 1:n()) %>%
  gather(key = "type", value = "temperature", 
         starts_with('mean_surf_'),
         starts_with('mean_bot_'), 
         starts_with('max_surf_'), 
         starts_with('max_bot_')) %>%
  separate(type, into=c('metric', 'depth', 'month'), sep='_')  %>%
  unite(metric, metric, depth) %>%
  spread(metric, temperature) %>%
 dplyr::select(-uniqueid) %>%
  group_by(wbic, year, month) %>%                      # average over sites
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup() %>%
  arrange(wbic, year, month) %>%
  mutate(date = ymd(paste(year, month, "15"))) %>%
  filter(date > "2003-01-01", date < "2014-05-01")
  
thermal_monthly <- thermal_monthly %>%
  group_by(wbic, month) %>%
  mutate(max_bot_z = scale(max_bot),                   # make z-score vars
         max_surf_z = scale(max_surf),
         mean_bot_z = scale(mean_bot),
         mean_surf_z = scale(mean_surf)) %>%
  ungroup() %>%
  mutate(layer_diff = mean_surf - mean_bot,            # create additional temp/time features
         quadratic_temp = mean_surf^2,
         season = fct_collapse(month,
                               "winter" = "dec",
                               "winter" = "jan",
                               "winter" = "feb",
                               "spring" = "mar",
                               "spring" = "apr",
                               "spring" = "may",
                               "summer" = "jun",
                               "summer" = "jul",
                               "summer" = "aug",
                               "fall"   = "sep",
                               "fall"   = "oct",
                               "fall"   = "nov"))
```

Add `temp` column via PCA to collapse down collinear covariates.

```{r}
pr <- thermal_monthly %>%
  dplyr::select(max_surf, mean_surf, mean_bot) %>%
  prcomp(center = TRUE, scale = TRUE)

temp <- thermal_monthly %>%
  dplyr::select(max_surf, mean_surf, mean_bot) %>%
  as.matrix() %*% pr$rotation[,1]

thermal_monthly <- thermal_monthly %>%
  add_column(temp)

rm(pr)
rm(temp)
```

```{r}
rm(thermal)
```

## Merging MME into thermal

There are some lakes where MMEs were recorded for which there is no available
modeled thermal metric data. Consequently, these cases are dropped in the merge.

```{r}
df <- thermal_monthly %>%
  left_join(thermal_annual, by = c("year", "wbic")) %>%
  left_join(MME, by = c("year", "month", "wbic"))

df <- df %>%
  mutate(summerkill = ifelse(is.na(summerkill), 0, summerkill),
         winterkill = ifelse(is.na(winterkill), 0, winterkill),
         anthropogenic = ifelse(is.na(anthropogenic), 0, anthropogenic),
         infectious = ifelse(is.na(infectious), 0, infectious),
         unknown = ifelse(is.na(unknown), 0, unknown))

rm(thermal_monthly, thermal_annual)
```


## Add spatial data

First the spatial covariates of the lakes.

```{r}
spatial <- readOGR("lake_shapes", layer = 'model_lakes')
spatial_df <- coordinates(spatial) %>%
  as.data.frame() %>%
  add_column(site_id = as.character(spatial@data$site_id)) %>%
  inner_join(NHD_WBIC, by = "site_id") %>%
  rename(lat = V2,
         lon = V1) %>%
  dplyr::select(lat, lon, wbic)

# Represent each lake by just one lat/lon
spatial_df <- spatial_df %>%
  group_by(wbic) %>%
  summarize(lon = mean(lon),
            lat = mean(lat))

rm(spatial)
```

Merge into existing data.

```{r}
df <- df %>%
  inner_join(spatial_df, by = "wbic")
```

Next, the 2010 census block covariates from https://www.census.gov/geo/maps-data/data/tiger-data.html

```{r}
census <- readOGR("census_data", layer = 'tabblock2010_55_pophu')
census_df <- coordinates(census) %>%
  as.data.frame() %>%
  add_column(population = census@data$POP10) %>%
  rename(lat = V2,
         lon = V1) %>%
  mutate(lon_round = round(lon, 1),
         lat_round = round(lat, 1)) %>%
  group_by(lon_round, lat_round) %>%
  summarize(population = sum(population)) %>%
  dplyr::select(lon_round, lat_round, population)
  
rm(census)
```

Join population data into main df by matching on the rounded lat/lon.

```{r}
df <- df %>%
  mutate(lon_round = round(lon, 1),
         lat_round = round(lat, 1)) %>%
  left_join(census_df, by = c("lon_round", "lat_round")) %>%
  dplyr::select(-lon_round, -lat_round)
```


## Write out processed historical data

```{r}
write_csv(df, "../processed/historical_data.csv")
```

```{r}
rm(df)
```




# Part II: Future Data

To make a dataset containing the information needed to predict MMEs into the future, we read in a new thermals data set and process it in the same way that we did the historical data. We then merge in the same spatial and census data since it is lake-specific.

Since the cleaning steps are identical to the above, we remove the commentary.

```{r}
thermal <- read_tsv('ACCESS_thermal_metrics.tsv') %>%
  inner_join(NHD_WBIC, by = "site_id")
```

```{r}
thermal <- thermal %>%
  rename_all(tolower) %>%
  filter(year >= 2013) %>%
  mutate(year = as.character(year)) %>%
 dplyr::select(-contains("strat"),
         -sthermo_depth_mean)
```

```{r}
thermal_annual <- thermal %>%
  dplyr::select(-contains('jas'),
                -starts_with('mean_surf_'),
                -starts_with('mean_bot_'), 
                -starts_with('max_surf_'), 
                -starts_with('max_bot_')) %>%
  group_by(wbic, year) %>%
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup() %>%
  rename(ice_duration = ice_duration_days,
         schmidt = schmidt_daily_annual_sum,
         variance_after_ice_30 = coef_var_0.30, 
         variance_after_ice_60 = coef_var_30.60, 
         cumulative_above_0 = gdd_wtr_0c,
         cumulative_above_5 = gdd_wtr_5c,
         cumulative_above_10 = gdd_wtr_10c) %>%
  mutate(log_schmidt = log(schmidt + .00001))
```

```{r}
thermal_monthly <- thermal %>%
  dplyr::select(starts_with('mean_surf_'),            # tidy the temp data
                starts_with('mean_bot_'), 
                starts_with('max_surf_'), 
                starts_with('max_bot_'),
                -contains('jas'),
                year, wbic) %>%
  mutate(uniqueid = 1:n()) %>%
  gather(key = "type", value = "temperature", 
         starts_with('mean_surf_'),
         starts_with('mean_bot_'), 
         starts_with('max_surf_'), 
         starts_with('max_bot_')) %>%
  separate(type, into=c('metric', 'depth', 'month'), sep='_')  %>%
  unite(metric, metric, depth) %>%
  spread(metric, temperature) %>%
 dplyr::select(-uniqueid) %>%
  group_by(wbic, year, month) %>%                      # average over sites
  summarise_if(is.numeric, mean, na.rm = TRUE) %>%
  ungroup() %>%
  arrange(wbic, year, month) %>%
  mutate(date = ymd(paste(year, month, "15"))) %>%
  filter(date > "2014-05-01")
  
thermal_monthly <- thermal_monthly %>%
  group_by(wbic, month) %>%
  mutate(max_bot_z = scale(max_bot),                   # make z-score vars
         max_surf_z = scale(max_surf),
         mean_bot_z = scale(mean_bot),
         mean_surf_z = scale(mean_surf)) %>%
  ungroup() %>%
  mutate(layer_diff = mean_surf - mean_bot,            # create additional temp/time features
         quadratic_temp = mean_surf^2,
         season = fct_collapse(month,
                               "winter" = "dec",
                               "winter" = "jan",
                               "winter" = "feb",
                               "spring" = "mar",
                               "spring" = "apr",
                               "spring" = "may",
                               "summer" = "jun",
                               "summer" = "jul",
                               "summer" = "aug",
                               "fall"   = "sep",
                               "fall"   = "oct",
                               "fall"   = "nov"))
```

```{r}
pr <- thermal_monthly %>%
  dplyr::select(max_surf, mean_surf, mean_bot) %>%
  prcomp(center = TRUE, scale = TRUE)

temp <- thermal_monthly %>%
  dplyr::select(max_surf, mean_surf, mean_bot) %>%
  as.matrix() %*% pr$rotation[,1]

thermal_monthly <- thermal_monthly %>%
  add_column(temp)

rm(pr)
rm(temp)
```

```{r}
rm(thermal)
```

```{r}
df <- thermal_monthly %>%
  left_join(thermal_annual, by = c("year", "wbic")) %>%
  left_join(spatial_df, by = "wbic") %>%
  mutate(lon_round = round(lon, 1),
         lat_round = round(lat, 1)) %>%
  left_join(census_df, by = c("lon_round", "lat_round")) %>%
  dplyr::select(-lon_round, -lat_round)

rm(thermal_monthly, thermal_annual, spatial_df, census_df)
```

```{r}
write_csv(df, "../processed/future_data.csv")
```

```{r}
rm(df)
```


# Snowfall Data

Load in snowfall data. Note that this code hasn't yet been fully tidied but still works just fine.

```{r}
tidy_snow <- function(path) {
  
  e <-extent(-92.9, -87, 42.4 , 46.9)
  a <- crop(raster(path),e)
  
  a1 <- as.data.frame(coordinates(a))
  a2<- as.data.frame(a)
  
  data <- na.omit(cbind(a1, a2)) 
   
  names(data) <- c('x', 'y', 'snow')

  data$long_round <- round(data$x, 1)
  data$lat_round <- round(data$y, 1)  

  data_output <- data %>%
    group_by(long_round, lat_round) %>%
    summarise(Snow = mean(snow))
    
  data_output$Year <- str_sub(path, 24, 27)
  data_output$Month <- str_sub(path, 28, 29)
  return(data_output)
}
```

```{r}
setwd("PRISM_precip")
file_names <- list.files() %>%
  str_subset("asc$")
snow_data <- map_df(file_names, tidy_snow)
```

```{r}
write_csv(snow_data, "../../processed/snow_data.csv")

rm(snow_data)
```