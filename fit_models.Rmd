---
title: "Fit Models"
author: "Aaron Till and Andrew Bray"
output: html_document
---

```{r}
library(tidyverse)
library(rsample)
library(caret)
library(glmnet)
library(Matrix)
library(e1071)
library(pROC)
library(PRROC) 
library(glmnetUtils)
```

## Prepare data

```{r}
setwd("data/processed/")
historical_data <- read_csv("historical_data.csv",
                            col_types = list(wbic = col_factor(NULL),
                                             month  = col_factor(NULL),
                                             season = col_factor(NULL),
                                             summerkill = col_factor(NULL),
                                             ice_duration = col_double())) %>%
  select(-cause.category.4)

future_data <- read_csv("future_data.csv",
                            col_types = list(wbic = col_factor(NULL),
                                             month  = col_factor(NULL),
                                             season = col_factor(NULL),
                                             ice_duration = col_double()))
```

```{r}
set.seed(998)
in_training <- createDataPartition(historical_data$summerkill,
                                  p = .75, list = FALSE)

training <- historical_data %>%
  mutate(summerkill = fct_recode(summerkill,
                                 "neg" = "0",
                                 "pos" = "1")) %>%
  slice(in_training)
testing  <- historical_data %>%
  mutate(summerkill = fct_recode(summerkill,
                                 "neg" = "0",
                                 "pos" = "1")) %>%
  slice(-in_training)
```

In order to compare coefficients, we center and scale the predictors.

```{r}
pre_proc_values <- preProcess(training, method = c("center", "scale"))
training <- predict(pre_proc_values, training)
testing  <- predict(pre_proc_values, testing)
```


## Specify variable sets

The first generalized model that we will consider is one using a subset of variables
selected based on their presumed effect and being mutually non-correlated (at least
strongly).

```{r}
f1 <- summerkill ~ variance_after_ice_30 + variance_after_ice_60 + log_schmidt +
  cumulative_above_10 + ice_duration + population + lon + lat + season + temp
```

The second model involves all covariates that we have at our disposal (except wbic,
which we treat separately because of the number of levels).

```{r}
f2 <- summerkill ~ variance_after_ice_30 + variance_after_ice_60 + log_schmidt +
  cumulative_above_10 + ice_duration + population + lon + lat + season + temp +
  max_bot + max_surf + mean_bot + mean_surf + max_bot_z + max_surf_z + mean_bot_z +
  mean_surf_z + layer_diff + quadratic_temp + peak_temp + cumulative_above_0 +
  cumulative_above_5
```


## Lasso Logistic

We first fit LASSO models with many difference lambdas, utilizing a 5-fold CV
scheme, repeated once, on the training data.

### f1, logloss, no downsampling

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 2,
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)

library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

par_grid <-  expand.grid(alpha = 1,
                        lambda = seq(1.8e-6, 3.5e-6, 
                                     length.out = 4))


set.seed(825)
lasso_fit_1 <- train(f1, 
                     data = training, 
                     method = "glmnet", 
                     metric = "mnLogLoss",
                     tuneGrid = par_grid,
                     trControl = fitControl)
stopCluster(cl)
write_rds(lasso_fit_1, "../models/lasso_f1_logloss.rds")
```

### f1, logloss, with downsampling

```{r}
fitControl <- trainControl(method = "repeatedcv",
                           number = 5,
                           repeats = 2,
                           summaryFunction = mnLogLoss,
                           classProbs = TRUE)

library(doParallel)
cl <- makePSOCKcluster(6)
registerDoParallel(cl)

# par_grid <-  expand.grid(alpha = 1,
#                         lambda = seq(1.8e-6, 3.5e-6, 
#                                      length.out = 4))


set.seed(825)
lasso_fit_1 <- train(f1, 
                     data = training, 
                     method = "glmnet", 
                     metric = "mnLogLoss",
                     #tuneGrid = par_grid,
                     trControl = fitControl)
stopCluster(cl)
write_rds(lasso_fit_1, "../models/lasso_f1_logloss_downsampled.rds")
```


## Ridge logistic

### f2, logloss, no downsampling

```{r}
par_grid <-  expand.grid(alpha = 0,
                        lambda = seq(1.8e-8, 3.5e-6, 
                                     length.out = 8))
cl <- makePSOCKcluster(8)
registerDoParallel(cl)

set.seed(825)
ridge_fit_2 <- train(f2, 
                     data = training, 
                     method = "glmnet", 
                     trControl = fitControl)
stopCluster(cl)
write_rds(ridge_fit_2, "../models/ridge_f1_logloss.rds")
```

